{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e53d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the codebase for VASARI-auto\n",
    "###\tvasari-auto.py | a pipeline for automated VASARI characterisation of glioma.\n",
    "###\tCopyright 2024 James Ruffle, High-Dimensional Neurology, UCL Queen Square Institute of Neurology.\n",
    "###\tThis program is licensed under the APACHE 2.0 license.\n",
    "###\tThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  \n",
    "### This program is not intended for clinical use of any kind.\n",
    "###\tSee the License for more details.\n",
    "###\tThis code is part of the repository https://github.com/james-ruffle/vasari-auto\n",
    "###\tCorrespondence to Dr James K Ruffle by email: j.ruffle@ucl.ac.uk\n",
    "\n",
    "#Import packages\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import errno\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import nibabel as nib\n",
    "from scipy.ndimage import label, generate_binary_structure\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage, misc\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import *\n",
    "import time\n",
    "from skimage.morphology import skeletonize\n",
    "import matplotlib.ticker as mticker\n",
    "from scipy import stats\n",
    "pd.set_option('display.max_rows', 500)\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def get_vasari_features(file,atlases='/home/jruffle/OneDrive/PhD/VASARI/code/vasari-auto/atlas_masks/',verbose=False,enhancing_label=3,nonenhancing_label=1,oedema_label=2,z_dim=-1,cf=1,t_ependymal=5000,t_wm=100,resolution=1,midline_thresh=5,enh_quality_thresh=15,cyst_thresh=50,cortical_thresh=1000,focus_thresh=30000,num_components_bin_thresh=10,num_components_cet_thresh=15):\n",
    "    \"\"\"\n",
    "    #Required argument\n",
    "    file - NIFTI segmentation file with binary lesion labels\n",
    "    atlases - atlas path for location derivation\n",
    "    \n",
    "    #Optional hyperparmeters\n",
    "    verbose - whether to enable verbose logging, default=False\n",
    "    enhancing_label - the integer value of enhancing tumour within file, default=3\n",
    "    nonenhancing_label - the integer value of nonenhancing tumour within file, default=1\n",
    "    oeedema_label - the integer value of nonenhancing tumour within file, default=2\n",
    "    z_dim - the dimension of the Z axis within file, default=-1, which assumes MNI template registration\n",
    "    cf - correction factor for ambiguity in voxel quantification, default=1\n",
    "    t_ependymal - threshold for lesion involvement within the ependyma, this can be customised depending on the voxel resolution you are operating in, default=5000\n",
    "    t_wm - threshold for lesion involvement within the wm, this can be customised depending on the voxel resolution you are operating in, default=100\n",
    "    resolution - volumetric voxel resolution, this is important for derivation of F11 - thickness of enhancing margin, default=1 (1mm x 1mm x 1mm resolution)\n",
    "    midline_thresh - threshold for number of diseased voxels that can cross the midline to be quantified as a lesion definitively crossing the midline, default=5\n",
    "    enh_quality_thresh - threshold for determining the quality of lesion enhancement by volume approximation. Please note ideally this feature would utilise source imaging but in its prototype format uses anonymised segmentation data only, default=15\n",
    "    cyst_thresh - threshold for determining the presence of cysts based on a heuristic of nCET detection, default=50\n",
    "    cortical_thresh - threshold for determining cortex involvement, default=1000\n",
    "    focus_thresh - threshold for determining a principle side of involvement, this will vary depending on resolution, default=30000\n",
    "    num_components_bin_thresh - threshold for quantifying a multifocal lesion, default = 10\n",
    "    num_components_cet_thresh - threshold for satellite lesions, default=15\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    if verbose:\n",
    "        print('Please note that this software is in beta and utilises only irrevocably anonymised lesion masks.\\nVASARI features that require source data shall not be derived and return NaN in this software version')\n",
    "        print('')\n",
    "        print('Working on: '+str(file))\n",
    "        print('')\n",
    "\n",
    "    ##derive anatomy masks - this is for automated location (F1)\n",
    "    brainstem = atlases+'brainstem.nii.gz'\n",
    "    brainstem = nib.load(brainstem)\n",
    "    brainstem_array = np.asanyarray(brainstem.dataobj)\n",
    "    brainstem_vol = np.sum(brainstem_array)\n",
    "    \n",
    "    frontal_lobe = atlases+'frontal_lobe.nii.gz'\n",
    "    frontal_lobe = nib.load(frontal_lobe)\n",
    "    frontal_lobe_array = np.asanyarray(frontal_lobe.dataobj)\n",
    "    frontal_lobe_vol = np.sum(frontal_lobe_array)\n",
    "    \n",
    "    insula = atlases+'insula.nii.gz'\n",
    "    insula = nib.load(insula)\n",
    "    insula_array = np.asanyarray(insula.dataobj)\n",
    "    insula_vol = np.sum(insula_array)\n",
    "    \n",
    "    occipital = atlases+'occipital.nii.gz'\n",
    "    occipital = nib.load(occipital)\n",
    "    occipital_array = np.asanyarray(occipital.dataobj)\n",
    "    occipital_vol = np.sum(occipital_array)\n",
    "    \n",
    "    parietal = atlases+'parietal.nii.gz'\n",
    "    parietal = nib.load(parietal)\n",
    "    parietal_array = np.asanyarray(parietal.dataobj)\n",
    "    parietal_vol = np.sum(parietal_array)\n",
    "    \n",
    "    temporal = atlases+'temporal.nii.gz'\n",
    "    temporal = nib.load(temporal)\n",
    "    temporal_array = np.asanyarray(temporal.dataobj)\n",
    "    temporal_vol = np.sum(temporal_array)\n",
    "    \n",
    "    thalamus = atlases+'thalamus.nii.gz'\n",
    "    thalamus = nib.load(thalamus)\n",
    "    thalamus_array = np.asanyarray(thalamus.dataobj)\n",
    "    thalamus_vol = np.sum(thalamus_array)\n",
    "    \n",
    "    corpus_callosum = atlases+'corpus_callosum.nii.gz'\n",
    "    corpus_callosum = nib.load(corpus_callosum)\n",
    "    corpus_callosum_array = np.asanyarray(corpus_callosum.dataobj)\n",
    "    corpus_callosum_vol = np.sum(corpus_callosum_array)\n",
    "    \n",
    "    ventricles = atlases+'ventricles.nii.gz'\n",
    "    ventricles = nib.load(ventricles)\n",
    "    ventricles_array = np.asanyarray(ventricles.dataobj)\n",
    "    ventricles_vol = np.sum(ventricles_array)\n",
    "    \n",
    "    internal_capsule = atlases+'internal_capsule.nii.gz'\n",
    "    internal_capsule = nib.load(internal_capsule)\n",
    "    internal_capsule_array = np.asanyarray(internal_capsule.dataobj)\n",
    "    internal_capsule_vol = np.sum(internal_capsule_array)\n",
    "    \n",
    "    cortex = atlases+'cortex.nii.gz'\n",
    "    cortex = nib.load(cortex)\n",
    "    cortex_array = np.asanyarray(cortex.dataobj)\n",
    "    cortex_vol = np.sum(cortex_array)\n",
    "    \n",
    "    segmentation = nib.load(file)\n",
    "    segmentation_array = np.asanyarray(segmentation.dataobj)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Running voxel quantification per tissue class')\n",
    "    total_lesion_burden = np.count_nonzero(segmentation_array)\n",
    "    enhancing_voxels = np.count_nonzero(segmentation_array == enhancing_label)\n",
    "    nonenhancing_voxels = np.count_nonzero(segmentation_array == nonenhancing_label)\n",
    "    oedema_voxels = np.count_nonzero(segmentation_array == oedema_label)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Deriving number of components')\n",
    "    labeled_array, num_components = label(segmentation_array)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Determining laterality')\n",
    "        #print('Note - if experiencing unexpected axis flipping for lesion laterality, check lesion registration space. This code assumes MNI template registration')\n",
    "    temp = segmentation_array.nonzero()[0]\n",
    "    right_hemisphere=len(temp[temp<int(segmentation_array.shape[z_dim]/2)])\n",
    "    left_hemisphere=len(temp[temp>int(segmentation_array.shape[z_dim]/2)])\n",
    "    if right_hemisphere>left_hemisphere:\n",
    "        side='Right'\n",
    "    if right_hemisphere<left_hemisphere:\n",
    "        side='Left'\n",
    "    if right_hemisphere>focus_thresh and left_hemisphere>focus_thresh:\n",
    "        side='Bilateral'\n",
    "    # if verbose:\n",
    "        # print(right_hemisphere)\n",
    "        # print(left_hemisphere)\n",
    "\n",
    "    if verbose:\n",
    "        print('Determining proportions')\n",
    "    segmentation_array[segmentation_array==oedema_label]=0\n",
    "    segmentation_array[segmentation_array==enhancing_label]=1\n",
    "    segmentation_array[segmentation_array==nonenhancing_label]=1\n",
    "    \n",
    "    if segmentation_array.sum()==0:\n",
    "        if verbose:\n",
    "            print('No lesion detected, falling back to oedema label for closer inspection')\n",
    "        segmentation_array = np.asanyarray(segmentation.dataobj)\n",
    "        segmentation_array[segmentation_array!=oedema_label]=0\n",
    "\n",
    "    prop_in_brainstem = len((segmentation_array*brainstem_array).nonzero()[0])/(segmentation_array.sum()+cf)\n",
    "    prop_in_frontal_lobe = len((segmentation_array*frontal_lobe_array).nonzero()[0])/(segmentation_array.sum()+cf)\n",
    "    prop_in_insula = len((segmentation_array*insula_array).nonzero()[0])/(segmentation_array.sum()+cf)\n",
    "    prop_in_occipital = len((segmentation_array*occipital_array).nonzero()[0])/(segmentation_array.sum()+cf)\n",
    "    prop_in_parietal = len((segmentation_array*parietal_array).nonzero()[0])/(segmentation_array.sum()+cf)\n",
    "    prop_in_temporal = len((segmentation_array*temporal_array).nonzero()[0])/(segmentation_array.sum()+cf)\n",
    "    prop_in_thalamus = len((segmentation_array*thalamus_array).nonzero()[0])/(segmentation_array.sum()+cf)\n",
    "    prop_in_cc = len((segmentation_array*corpus_callosum_array).nonzero()[0])/(segmentation_array.sum()+cf)\n",
    "\n",
    "    d = {'ROI': ['Brainstem','Frontal Lobe','Insula','Occipital Lobe','Parietal Lobe','Temporal Lobe','Thalamus','Corpus callosum'], \n",
    "         'prop': [prop_in_brainstem,prop_in_frontal_lobe,prop_in_insula,prop_in_occipital,prop_in_parietal,prop_in_temporal,prop_in_thalamus,prop_in_cc]}\n",
    "    \n",
    "    vols = pd.DataFrame(data=d)\n",
    "    vols = vols.sort_values(by='prop',ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(vols)\n",
    "    \n",
    "    proportion_enhancing = (enhancing_voxels/total_lesion_burden)*100\n",
    "#     proportion_enhancing = (enhancing_voxels/(enhancing_voxels+nonenhancing_voxels+.1))*100\n",
    "    proportion_nonenhancing = (nonenhancing_voxels/total_lesion_burden)*100\n",
    "#     proportion_nonenhancing = (nonenhancing_voxels/(enhancing_voxels+nonenhancing_voxels+.1))*100\n",
    "    proportion_oedema = (oedema_voxels/total_lesion_burden+.1)*100\n",
    "    \n",
    "    enhancement_quality = 1\n",
    "    if proportion_enhancing>0: #heuristic of if model segments more than 10% voxels are enhancing\n",
    "        if proportion_enhancing>enh_quality_thresh:\n",
    "            enhancement_quality=3\n",
    "        else:\n",
    "            enhancement_quality=2\n",
    "    \n",
    "    if verbose:\n",
    "        print('Determining ependymal involvement')\n",
    "    if len((segmentation_array*ventricles_array).nonzero()[0])>=t_ependymal:\n",
    "        ependymal=2\n",
    "\n",
    "    if len((segmentation_array*ventricles_array).nonzero()[0])<t_ependymal:\n",
    "        ependymal=1\n",
    "\n",
    "    if verbose:\n",
    "        print('Determining white matter involvemenet')\n",
    "    deep_wm='None'\n",
    "    if len((segmentation_array*brainstem_array).nonzero()[0])>=t_wm:\n",
    "        deep_wm='Brainstem'\n",
    "\n",
    "    if len((segmentation_array*corpus_callosum_array).nonzero()[0])>=t_wm:\n",
    "        deep_wm='Corpus Callosum'\n",
    "\n",
    "    if len((segmentation_array*internal_capsule_array).nonzero()[0])>=t_wm:\n",
    "        deep_wm='Internal Capsule'\n",
    "    deep_wm_f = np.nan\n",
    "    if deep_wm=='None':\n",
    "        deep_wm_f=1\n",
    "    if deep_wm!='None':\n",
    "        deep_wm_f=2\n",
    "        \n",
    "    if verbose:\n",
    "        print('Determining cortical involvement')\n",
    "\n",
    "    cortical_lesioned_voxels = len((segmentation_array*cortex_array).nonzero()[0])\n",
    "    cortical_lesioned_voxels_f = np.nan\n",
    "    if cortical_lesioned_voxels>cortical_thresh:\n",
    "        cortical_lesioned_voxels_f=2\n",
    "    if cortical_lesioned_voxels<=cortical_thresh:\n",
    "        cortical_lesioned_voxels_f=1\n",
    "    if verbose:\n",
    "        print('Cortically lesioned voxels '+str(cortical_lesioned_voxels))\n",
    "\n",
    "    if verbose:\n",
    "        print('Determining midline involvement')\n",
    "    nCET_cross_midline=False\n",
    "    nCET = np.asanyarray(segmentation.dataobj)\n",
    "    nCET[nCET!=nonenhancing_label]=0\n",
    "    nCET[nCET>0]=1\n",
    "    temp = nCET.nonzero()[0]\n",
    "    right_hemisphere=len(temp[temp<int(segmentation_array.shape[z_dim]/2)])\n",
    "    left_hemisphere=len(temp[temp>int(segmentation_array.shape[z_dim]/2)])\n",
    "    if right_hemisphere>midline_thresh and left_hemisphere>midline_thresh:\n",
    "        nCET_cross_midline=True\n",
    "    nCET_cross_midline_f = np.nan\n",
    "    if nCET_cross_midline==True:\n",
    "        nCET_cross_midline_f=3\n",
    "    if nCET_cross_midline==False:\n",
    "        nCET_cross_midline_f=2\n",
    "    \n",
    "    CET_cross_midline=False\n",
    "    CET = np.asanyarray(segmentation.dataobj)\n",
    "    CET[CET!=enhancing_label]=0\n",
    "    CET[CET>0]=1\n",
    "    temp = CET.nonzero()[0]\n",
    "    right_hemisphere=len(temp[temp<int(segmentation_array.shape[z_dim]/2)])\n",
    "    left_hemisphere=len(temp[temp>int(segmentation_array.shape[z_dim]/2)])\n",
    "    if right_hemisphere>midline_thresh and left_hemisphere>midline_thresh:\n",
    "        CET_cross_midline=True\n",
    "    CET_cross_midline_f = np.nan\n",
    "    if CET_cross_midline==True:\n",
    "        CET_cross_midline_f=3\n",
    "    if CET_cross_midline==False:\n",
    "        CET_cross_midline_f=2\n",
    "\n",
    "    if verbose:\n",
    "        print('Deriving enhancing satellites')\n",
    "    labeled_array, num_components_cet = label(CET)\n",
    "    num_components_cet_f = np.nan\n",
    "    if num_components_cet>num_components_cet_thresh:\n",
    "        num_components_cet_f =2\n",
    "    else:\n",
    "        num_components_cet_f=1\n",
    "    \n",
    "    if verbose:\n",
    "        print('Deriving cysts')\n",
    "    #heuristic to approximate cyst formation by multifocal nonenhancing components\n",
    "    labeled_array, num_components_ncet = label(nCET)\n",
    "    num_components_ncet_f =1\n",
    "    if num_components_ncet>cyst_thresh:\n",
    "        num_components_ncet_f=2\n",
    "        \n",
    "    if verbose:\n",
    "        print('Cyst count '+str(num_components_ncet))\n",
    "        \n",
    "    if verbose:\n",
    "        print('Deriving enhancement thickness')\n",
    "        \n",
    "    \n",
    "    enhancing_skeleton = skeletonize(CET)\n",
    "    allpixels = np.count_nonzero(CET)\n",
    "    skeletonpixels = np.count_nonzero(enhancing_skeleton)\n",
    "    \n",
    "    if allpixels>0:\n",
    "        enhancing_thickness = allpixels/(skeletonpixels+1e-9)\n",
    "    if allpixels==0:\n",
    "        enhancing_thickness=0\n",
    "    enhancing_thickness_f = np.nan\n",
    "    ll=3\n",
    "    if enhancing_thickness<ll:\n",
    "        enhancing_thickness_f=3\n",
    "    if enhancing_thickness>=ll:\n",
    "        enhancing_thickness_f=4\n",
    "    if enhancing_thickness>=ll and nonenhancing_voxels==0:\n",
    "        enhancing_thickness_f=5\n",
    "        \n",
    "    if verbose:\n",
    "        print('Enhancing thickness: '+str(enhancing_thickness))\n",
    "    \n",
    "    if verbose:\n",
    "        print('Converting raw values to VASARI dictionary features')\n",
    "    F1_dict = {'Frontal Lobe':1,'Temporal Lobe':2,'Insula':3,'Parietal Lobe':4,'Occipital Lobe':5,'Brainstem':6,'Corpus callosum':7,'Thalamus':8}\n",
    "    F2_dict = {'Right':1,'Left':3,'Bilateral':2}\n",
    "\n",
    "    proportion_enhancing_f = np.nan\n",
    "    if proportion_enhancing<=5:\n",
    "        proportion_enhancing_f = 3\n",
    "    if 5 < proportion_enhancing <= 33:\n",
    "        proportion_enhancing_f = 4\n",
    "    if 33 < proportion_enhancing <= 67:\n",
    "        proportion_enhancing_f = 5\n",
    "    if 67 < proportion_enhancing <= 100:\n",
    "        proportion_enhancing_f = 6\n",
    "        \n",
    "    proportion_nonenhancing_f = np.nan\n",
    "    if proportion_nonenhancing<=5:\n",
    "        proportion_nonenhancing_f = 3\n",
    "    if 5 < proportion_nonenhancing <= 33:\n",
    "        proportion_nonenhancing_f = 4\n",
    "    if 33 < proportion_nonenhancing <= 67:\n",
    "        proportion_nonenhancing_f = 5\n",
    "    if 67 < proportion_nonenhancing <= 95:\n",
    "        proportion_nonenhancing_f = 6  \n",
    "    if 95 < proportion_nonenhancing <= 99.5:\n",
    "        proportion_nonenhancing_f = 7\n",
    "    if proportion_nonenhancing >99.5: #allow for small segmentation variation\n",
    "        proportion_nonenhancing_f = 8 \n",
    "        \n",
    "    proportion_necrosis_f = np.nan\n",
    "    if proportion_nonenhancing==0:\n",
    "        proportion_necrosis_f=2\n",
    "    if 0<proportion_nonenhancing<=5:\n",
    "        proportion_necrosis_f=3\n",
    "    if 5<proportion_nonenhancing<=33:\n",
    "        proportion_necrosis_f=4\n",
    "    if 33<proportion_nonenhancing<=67:\n",
    "        proportion_necrosis_f=5\n",
    "        \n",
    "    segmentation_array_binary = segmentation_array.copy()\n",
    "    segmentation_array_binary[segmentation_array_binary>0]=1\n",
    "    labeled_array_bin, num_components_bin = label(segmentation_array_binary)\n",
    "    f9_multifocal = 1\n",
    "    if num_components_bin>num_components_bin_thresh:\n",
    "        f9_multifocal=2\n",
    "    if verbose:\n",
    "        print('Number of lesion components: '+str(num_components_bin))\n",
    "        \n",
    "    proportion_oedema_f=np.nan\n",
    "    if verbose:\n",
    "        print('prop oedema '+str(proportion_oedema))\n",
    "    \n",
    "    if proportion_oedema==0:\n",
    "        proportion_oedema_f=2\n",
    "    if 0<proportion_oedema<=5:\n",
    "        proportion_oedema_f=3\n",
    "    if 5<proportion_oedema<=33:\n",
    "        proportion_oedema_f=4\n",
    "    if 33<proportion_oedema:\n",
    "        proportion_oedema_f=5\n",
    "    \n",
    "        \n",
    "    end_time = time.time()\n",
    "    time_taken = (end_time - start_time)\n",
    "    if verbose:\n",
    "        print(\"Time taken: \"+ str(time_taken)+\" seconds\")\n",
    "        \n",
    "    if verbose:\n",
    "        print('')\n",
    "        print('Complete! Generating output...')\n",
    "        \n",
    "    col_names = ['filename', 'reporter', 'time_taken_seconds',\n",
    "           'F1 Tumour Location', 'F2 Side of Tumour Epicenter',\n",
    "           'F3 Eloquent Brain', 'F4 Enhancement Quality',\n",
    "           'F5 Proportion Enhancing', 'F6 Proportion nCET',\n",
    "           'F7 Proportion Necrosis', 'F8 Cyst(s)', 'F9 Multifocal or Multicentric',\n",
    "           'F10 T1/FLAIR Ratio', 'F11 Thickness of enhancing margin',\n",
    "           'F12 Definition of the Enhancing margin',\n",
    "           'F13 Definition of the non-enhancing tumour margin',\n",
    "           'F14 Proportion of Oedema', 'F16 haemorrhage', 'F17 Diffusion',\n",
    "           'F18 Pial invasion', 'F19 Ependymal Invasion',\n",
    "           'F20 Cortical involvement', 'F21 Deep WM invasion', \n",
    "                 'F22 nCET Crosses Midline', 'F23 CET Crosses midline',\n",
    "                 'F24 satellites',\n",
    "           'F25 Calvarial modelling', 'COMMENTS']\n",
    "        \n",
    "\n",
    "    result = pd.DataFrame(columns=col_names)\n",
    "    result.loc[len(result)] = {'filename':file,\n",
    "                               'reporter':'VASARI-auto',\n",
    "                              'time_taken_seconds':time_taken,\n",
    "                              'F1 Tumour Location':F1_dict[vols.iloc[0,0]], #vols.iloc[0,0],\n",
    "                              'F2 Side of Tumour Epicenter':F2_dict[side],\n",
    "                              'F3 Eloquent Brain':np.nan, #unsupported in current version\n",
    "                              'F4 Enhancement Quality':enhancement_quality,\n",
    "                              'F5 Proportion Enhancing':proportion_enhancing_f,\n",
    "                              'F6 Proportion nCET':proportion_nonenhancing_f,\n",
    "                              'F7 Proportion Necrosis':proportion_necrosis_f,\n",
    "                              'F8 Cyst(s)':num_components_ncet_f,\n",
    "                                'F9 Multifocal or Multicentric':f9_multifocal,\n",
    "                               'F10 T1/FLAIR Ratio':np.nan,  #unsupported in current version\n",
    "                               'F11 Thickness of enhancing margin':enhancing_thickness_f,\n",
    "                               'F12 Definition of the Enhancing margin':np.nan,  #unsupported in current version\n",
    "                               'F13 Definition of the non-enhancing tumour margin':np.nan,  #unsupported in current version\n",
    "                               'F14 Proportion of Oedema': proportion_oedema_f,\n",
    "                               'F16 haemorrhage':np.nan,  #unsupported in current version\n",
    "                               'F17 Diffusion':np.nan,  #unsupported in current version\n",
    "                               'F18 Pial invasion':np.nan, #unsupported in current version\n",
    "                               'F19 Ependymal Invasion':ependymal, \n",
    "                               'F20 Cortical involvement':cortical_lesioned_voxels_f,\n",
    "                               'F21 Deep WM invasion':deep_wm_f, \n",
    "                               'F22 nCET Crosses Midline':nCET_cross_midline_f,\n",
    "                               'F23 CET Crosses midline':CET_cross_midline_f,\n",
    "                               'F24 satellites':num_components_cet_f, \n",
    "                               'F25 Calvarial modelling':np.nan, #unsupported in current version\n",
    "                               'COMMENTS':'Please note that this software is in beta and utilises only irrevocably anonymised lesion masks. VASARI features that require source data shall not be derived and return NaN'\n",
    "                              }\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
